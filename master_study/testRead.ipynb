{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_maker\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminiary info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"example_tunescan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load job tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of output simulation files started\n"
     ]
    }
   ],
   "source": [
    "# Start of the script\n",
    "print(\"Analysis of output simulation files started\")\n",
    "start = time.time()\n",
    "\n",
    "# Load Data\n",
    "\n",
    "fix = \"/scans/\" + study_name\n",
    "root = tree_maker.tree_from_json(fix[1:] + \"/tree_maker_\" + study_name + \".json\")\n",
    "# Add suffix to the root node path to handle scans that are not in the root directory\n",
    "root.add_suffix(suffix=fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse simulations folder and extract relevant observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_problematic_sim = []\n",
    "l_df_to_merge = []\n",
    "for node in root.generation(1):\n",
    "    with open(f\"{node.get_abs_path()}/config.yaml\", \"r\") as fid:\n",
    "        config_parent = yaml.safe_load(fid)\n",
    "    for node_child in node.children:\n",
    "        with open(f\"{node_child.get_abs_path()}/config.yaml\", \"r\") as fid:\n",
    "            config_child = yaml.safe_load(fid)\n",
    "        try:\n",
    "            particle = pd.read_parquet(\n",
    "                f\"{node_child.get_abs_path()}/{config_child['config_simulation']['particle_file']}\"\n",
    "            )\n",
    "            df_sim = pd.read_parquet(f\"{node_child.get_abs_path()}/output_particles.parquet\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            l_problematic_sim.append(node_child.get_abs_path())\n",
    "            continue\n",
    "\n",
    "        # Register paths and names of the nodes\n",
    "        df_sim[\"path base collider\"] = f\"{node.get_abs_path()}\"\n",
    "        df_sim[\"name base collider\"] = f\"{node.name}\"\n",
    "        df_sim[\"path simulation\"] = f\"{node_child.get_abs_path()}\"\n",
    "        df_sim[\"name simulation\"] = f\"{node_child.name}\"\n",
    "\n",
    "        # Get node parameters as dictionnaries for parameter assignation\n",
    "        dic_child_collider = node_child.parameters[\"config_collider\"]\n",
    "        dic_child_simulation = node_child.parameters[\"config_simulation\"]\n",
    "        try:\n",
    "            dic_parent_collider = node.parameters[\"config_mad\"]\n",
    "        except:\n",
    "            print(\"No parent collider could be loaded\")\n",
    "        dic_parent_particles = node.parameters[\"config_particles\"]\n",
    "\n",
    "        # Get which beam is being tracked\n",
    "        df_sim[\"beam\"] = dic_child_simulation[\"beam\"]\n",
    "\n",
    "        # Get scanned parameters (complete with the requested scanned parameters)\n",
    "        df_sim[\"qx\"] = dic_child_collider[\"config_knobs_and_tuning\"][\"qx\"][\"lhcb1\"]\n",
    "        df_sim[\"qy\"] = dic_child_collider[\"config_knobs_and_tuning\"][\"qy\"][\"lhcb1\"]\n",
    "        df_sim[\"dqx\"] = dic_child_collider[\"config_knobs_and_tuning\"][\"dqx\"][\"lhcb1\"]\n",
    "        df_sim[\"dqy\"] = dic_child_collider[\"config_knobs_and_tuning\"][\"dqy\"][\"lhcb1\"]\n",
    "        df_sim[\"i_bunch_b1\"] = dic_child_collider[\"config_beambeam\"][\"mask_with_filling_pattern\"][\n",
    "            \"i_bunch_b1\"\n",
    "        ]\n",
    "        df_sim[\"i_bunch_b2\"] = dic_child_collider[\"config_beambeam\"][\"mask_with_filling_pattern\"][\n",
    "            \"i_bunch_b2\"\n",
    "        ]\n",
    "        df_sim[\"num_particles_per_bunch\"] = dic_child_collider[\"config_beambeam\"][\n",
    "            \"num_particles_per_bunch\"\n",
    "        ]\n",
    "\n",
    "        # Merge with particle data\n",
    "        df_sim_with_particle = pd.merge(df_sim, particle, on=[\"particle_id\"])\n",
    "        l_df_to_merge.append(df_sim_with_particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_mad': {'beam_config': {'lhcb1': {'beam_energy_tot': 6800},\n",
       "   'lhcb2': {'beam_energy_tot': 6800}},\n",
       "  'links': {'acc-models-lhc': '/afs/cern.ch/eng/lhc/optics/runIII'},\n",
       "  'optics_file': 'acc-models-lhc/RunIII_dev/Proton_2024/V0/opticsfile.40',\n",
       "  'ver_hllhc_optics': None,\n",
       "  'ver_lhc_run': 3.0},\n",
       " 'config_particles': {'n_angles': 5,\n",
       "  'n_r': 256,\n",
       "  'n_split': 5,\n",
       "  'r_max': 10,\n",
       "  'r_min': 2}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge outputs into dataframe (but don't save yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes from all simulations together\n",
    "df_all_sim = pd.concat(l_df_to_merge)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the particles that were lost for DA computation\n",
    "df_lost_particles = df_all_sim[df_all_sim[\"state\"] != 1]  # Lost particles\n",
    "\n",
    "# Check if the dataframe is empty\n",
    "if df_lost_particles.empty:\n",
    "    print(\"No unstable particles found, the output dataframe will be empty.\")\n",
    "\n",
    "# Group by working point (Update this with the knobs you want to group by !)\n",
    "group_by_parameters = [\"name base collider\", \"qx\", \"qy\"]\n",
    "# We always want to keep beam in the final result\n",
    "group_by_parameters = [\"beam\"] + group_by_parameters\n",
    "l_parameters_to_keep = [\n",
    "    \"normalized amplitude in xy-plane\",\n",
    "    \"qx\",\n",
    "    \"qy\",\n",
    "    \"dqx\",\n",
    "    \"dqy\",\n",
    "    \"i_bunch_b1\",\n",
    "    \"i_bunch_b2\",\n",
    "    \"num_particles_per_bunch\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as before, but call da2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe for current set of simulations:                                          normalized amplitude in xy-plane  \\\n",
      "beam  name base collider qx     qy                                         \n",
      "lhcb1 base_collider      62.305 60.309                              3.75   \n",
      "\n",
      "                                            qx      qy   dqx   dqy  \\\n",
      "beam  name base collider qx     qy                                   \n",
      "lhcb1 base_collider      62.305 60.309  62.305  60.309  15.0  15.0   \n",
      "\n",
      "                                        i_bunch_b1  i_bunch_b2  \\\n",
      "beam  name base collider qx     qy                               \n",
      "lhcb1 base_collider      62.305 60.309       411.0       410.0   \n",
      "\n",
      "                                        num_particles_per_bunch  \n",
      "beam  name base collider qx     qy                               \n",
      "lhcb1 base_collider      62.305 60.309             1.150000e+11  \n",
      "Elapsed time:  2912.177021741867\n"
     ]
    }
   ],
   "source": [
    "# Min is computed in the groupby function, but values should be identical\n",
    "my_final = pd.DataFrame(\n",
    "    [\n",
    "        df_lost_particles.groupby(group_by_parameters)[parameter].min()\n",
    "        for parameter in l_parameters_to_keep\n",
    "    ]\n",
    ").transpose()\n",
    "\n",
    "# Save data and print time\n",
    "my_final.to_parquet(f\"scans/{study_name}/da2.parquet\")\n",
    "print(\"Final dataframe for current set of simulations: \", my_final)\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_sim.to_parquet(f\"scans/{study_name}/all_sim.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['start_tracking_at_element', 'q0', 'mass0', 's', 'zeta', 'chi',\n",
       "       'charge_ratio', 'weight', 'particle_id', 'at_element', 'at_turn',\n",
       "       'state', 'parent_particle_id', '_rng_s1', '_rng_s2', '_rng_s3',\n",
       "       '_rng_s4', 'x', 'y', 'px', 'py', 'delta', 'ptau', 'rvv', 'rpp', 'p0c',\n",
       "       'beta0', 'gamma0', 'path base collider', 'name base collider',\n",
       "       'path simulation', 'name simulation', 'beam', 'qx', 'qy', 'dqx', 'dqy',\n",
       "       'i_bunch_b1', 'i_bunch_b2', 'num_particles_per_bunch',\n",
       "       'normalized amplitude in xy-plane', 'angle in xy-plane [deg]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lost_particles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe for current set of simulations:                                          normalized amplitude in xy-plane  \\\n",
      "beam  name base collider qx     qy                                         \n",
      "lhcb1 base_collider      62.305 60.309                              3.75   \n",
      "\n",
      "                                            qx      qy   dqx   dqy  \\\n",
      "beam  name base collider qx     qy                                   \n",
      "lhcb1 base_collider      62.305 60.309  62.305  60.309  15.0  15.0   \n",
      "\n",
      "                                        i_bunch_b1  i_bunch_b2  \\\n",
      "beam  name base collider qx     qy                               \n",
      "lhcb1 base_collider      62.305 60.309       411.0       410.0   \n",
      "\n",
      "                                        num_particles_per_bunch  \n",
      "beam  name base collider qx     qy                               \n",
      "lhcb1 base_collider      62.305 60.309             1.150000e+11  \n",
      "Elapsed time:  6643.716687679291\n"
     ]
    }
   ],
   "source": [
    "# Min is computed in the groupby function, but values should be identical\n",
    "my_full = pd.DataFrame(\n",
    "    [\n",
    "        df_lost_particles.groupby(group_by_parameters)[parameter].min()\n",
    "        for parameter in df_lost_particles.columns\n",
    "    ]\n",
    ").transpose()\n",
    "\n",
    "# Save data and print time\n",
    "my_full.to_parquet(f\"scans/{study_name}/da3.parquet\")\n",
    "print(\"Final dataframe for current set of simulations: \", my_final)\n",
    "end = time.time()\n",
    "print(\"Elapsed time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f\"scans/{study_name}/particles.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
